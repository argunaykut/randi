{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import andi\n",
    "import numpy as np\n",
    "import csv as csv\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras import losses, metrics\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.layers import LSTM\n",
    "\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Bidirectional\n",
    "#from keras.models import load_model\n",
    "from tensorflow.keras.models import load_model\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "#from data_split import data_split\n",
    "from utils import data_norm, data_reshape, many_net_uhd, my_atan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating trajectories to test the data. 3 trajectories for each dimension of inference and classification tasks. This requires the AnDi package, downloadable at https://github.com/AnDiChallenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a dataset for task(s) [1, 2, 3] and dimension(s) [1, 2, 3].\n",
      "Generating dataset for dimension 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/fbm/fbm.py:172: UserWarning: Combination of increments n and Hurst value H invalid for Davies-Harte method. Reverting to Hosking method. Occurs when n is small and Hurst is close to 1. \n",
      "  \"Combination of increments n and Hurst value H \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating dataset for dimension 2.\n",
      "Generating dataset for dimension 3.\n"
     ]
    }
   ],
   "source": [
    "AD = andi.andi_datasets()\n",
    "i=200\n",
    "\n",
    "X1, Y1, X2, Y2, X3, Y3 = AD.andi_dataset(N = 3, tasks = [1,2,3], dimensions = [1,2,3],\n",
    "                                        min_T = i, max_T = i+1, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference of 1d trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the inference networks is the inferred anomalous exponent for each trajectory. Trajectories must be of equal length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the networks used for inference in 1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers_inf_1d = [25, 50, 65, 75, 125, 165, 225, \n",
    "                  325, 425, 525, 625, 725, 825, 925]\n",
    "meta_model_inf_1d = []\n",
    "for i in centers_inf_1d: \n",
    "    m = load_model('nets/inference_nets/1d/inference_1D_'+str(i)+'.h5')\n",
    "    \n",
    "    meta_model_inf_1d.append(m)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the net trained on trajectories of length 165"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted exponents [1.6737214 1.2866617 0.9582677]\n",
      "ground truth [1.85, 1.25, 1.55]\n"
     ]
    }
   ],
   "source": [
    "#choosing the net\n",
    "net = meta_model_inf_1d[5]\n",
    "#finding out the block size used by the chosen net\n",
    "bs = net.layers[0].input_shape[-1]\n",
    "\n",
    "#normalizing the data\n",
    "data = data_norm(X1[0],dim=1,task=1)\n",
    "\n",
    "\n",
    "#reshaping the data\n",
    "\n",
    "data_rs = data_reshape(data,bs=bs,dim=1)\n",
    "#prediction on trajectories of length 200 using a net trained on traj of length 165\n",
    "pred_200_u165 = net.predict(data_rs)\n",
    "print('predicted exponents', pred_200_u165.flatten())\n",
    "print('ground truth', Y1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the net trained on trajectories of length 225"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted exponents [1.6827123 1.2324138 0.9565854]\n",
      "ground truth [1.85, 1.25, 1.55]\n"
     ]
    }
   ],
   "source": [
    "#choosing the net\n",
    "net = meta_model_inf_1d[6]\n",
    "#finding out the block size used by the chosen net\n",
    "bs = net.layers[0].input_shape[-1]\n",
    "\n",
    "#normalizing the data\n",
    "data = data_norm(X1[0],dim=1,task=1)\n",
    "\n",
    "\n",
    "#reshaping the data\n",
    "\n",
    "data_rs = data_reshape(data,bs=bs,dim=1)\n",
    "#prediction on trajectories of length 200 using a net trained on traj of length 225\n",
    "pred_200_u225 = net.predict(data_rs)\n",
    "print('predicted exponents', pred_200_u225.flatten())\n",
    "print('ground truth', Y1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the combination of nearest nets, which in this case is 165 and 225 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted exponents [1.678966   1.255017   0.95728636]\n",
      "ground truth [1.85, 1.25, 1.55]\n"
     ]
    }
   ],
   "source": [
    "pred_200_comb = many_net_uhd(nets = meta_model_inf_1d, traj_set = X1[0], centers = centers_inf_1d ,dim = 1, task =1)\n",
    "print('predicted exponents',pred_200_comb)\n",
    "print('ground truth', Y1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification in 2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the classiciation networks is an array giving the probability that the trajectory belongs to a model class. The classes, as in AnDi are [attm,ctrw,fbm,lw,sbm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers_class_2d = [25, 65, 125, 225, 425]\n",
    "meta_model_class_2d = []\n",
    "for i in centers_class_2d: \n",
    "    m = load_model('nets/classification_nets/2d/classification_2D_'+str(i)+'.h5')\n",
    "    \n",
    "    meta_model_class_2d.append(m)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability of each model class \n",
      " [[1.5193230e-09 1.0234864e-09 2.5234040e-06 9.9999750e-01 4.0343169e-09]\n",
      " [9.9036330e-01 4.4018007e-03 4.0434506e-06 5.2025188e-03 2.8314100e-05]\n",
      " [7.2885854e-03 1.8900841e-05 9.9123013e-01 7.8297366e-05 1.3840646e-03]]\n",
      "predicte most likely model\n",
      "lw\n",
      "attm\n",
      "fbm\n",
      "Ground truth\n",
      "lw\n",
      "attm\n",
      "fbm\n"
     ]
    }
   ],
   "source": [
    "#choosing the net\n",
    "net = meta_model_class_2d[2]\n",
    "#finding out the block size used by the chosen net\n",
    "bs = net.layers[0].input_shape[-1]\n",
    "\n",
    "#normalizing the data\n",
    "data = data_norm(X2[1],dim=2,task=2)\n",
    "\n",
    "\n",
    "#reshaping the data\n",
    "\n",
    "data_rs = data_reshape(data,bs=bs,dim=2)\n",
    "#prediction on trajectories of length 200 using a net trained on traj of length 165\n",
    "cla_200_u125 = net.predict(data_rs)\n",
    "print(\"probability of each model class\",'\\n',cla_200_u125)\n",
    "print(\"predicte most likely model\")\n",
    "models = ['attm', 'ctrw', 'fbm', 'lw', 'sbm'] \n",
    "for i in range(len(data)):\n",
    "    print(models[np.argmax(cla_200_u125[i])])\n",
    "print('Ground truth')    \n",
    "for i in range(len(data)):\n",
    "    print(models[int(Y2[1][i])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining the nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability of each model class \n",
      " [[4.0953418e-10 3.5874057e-10 7.7262825e-07 9.9999917e-01 1.0397374e-09]\n",
      " [9.9664122e-01 1.7458976e-03 1.3668625e-05 1.5421249e-03 5.7050122e-05]\n",
      " [5.1056454e-03 3.8738741e-05 9.9334824e-01 2.4466257e-04 1.2627979e-03]]\n",
      "most likely model\n",
      "lw\n",
      "attm\n",
      "fbm\n",
      "Ground truth\n",
      "lw\n",
      "attm\n",
      "fbm\n"
     ]
    }
   ],
   "source": [
    "cla_200_comb = many_net_uhd(nets = meta_model_class_2d, traj_set = X2[1], centers = centers_class_2d ,dim = 2, task =2)\n",
    "cla_200_comb = cla_200_comb.reshape(-1,5)\n",
    "print(\"probability of each model class\",'\\n',cla_200_comb)\n",
    "print(\"most likely model\")\n",
    "models = ['attm', 'ctrw', 'fbm', 'lw', 'sbm'] \n",
    "for i in range(len(data)):\n",
    "    print(models[np.argmax(cla_200_comb[i])])\n",
    "print('Ground truth')    \n",
    "for i in range(len(data)):\n",
    "    print(models[int(Y2[1][i])])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation in 3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network to infer the exponents of the two segments and the switching point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_t3d = tf.keras.models.load_model('nets/segmentation_nets/3d/T33D_inf.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network to classify the model of the second segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class2_3d = tf.keras.models.load_model('nets/segmentation_nets/3d/T33D_c2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is [a1,a2,sin(2pi*t/T),cos(2pi*t/T)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted first exponents [0.57030606 1.1435541  0.5654856 ]\n",
      "ground truth first exponents [1.25 1.7  0.25]\n",
      "predicted second exponents [1.2434977 1.4401271 1.4441462]\n",
      "ground truth second exponents [0.35 0.85 1.55]\n",
      "predicted time and exponents [195.83064288 138.5685842    4.47446404]\n",
      "ground truth time [184. 146.   1.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#finding out the block size used by the chosen net\n",
    "bs = model_a_t3d.layers[0].input_shape[-1]\n",
    "\n",
    "#normalizing the data\n",
    "data = data_norm(X3[2],dim=3,task=3)\n",
    "\n",
    "\n",
    "#reshaping the data\n",
    "\n",
    "data_rs = data_reshape(data,bs=bs,dim=3)\n",
    "#prediction on trajectories of length 200 using a net trained on traj of length 165\n",
    "inf_seg = model_a_t3d.predict(data_rs)\n",
    "pred_t = my_atan(inf_seg[:,2],inf_seg[:,3])*200/(2*np.pi)\n",
    "cl2_3d = class2_3d.predict(data_rs)\n",
    "\n",
    "print('predicted first exponents', inf_seg[:,0])\n",
    "print('ground truth first exponents', Y3[2][:,3])\n",
    "print('predicted second exponents', inf_seg[:,1])\n",
    "print('ground truth second exponents', Y3[2][:,5])\n",
    "print('predicted time and exponents', pred_t)\n",
    "print('ground truth time', Y3[2][:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability of each model class for second segment \n",
      " [[7.2716689e-03 4.0842197e-04 1.9752480e-01 4.7767358e-03 7.9001844e-01]\n",
      " [4.9542618e-04 3.6407507e-04 1.6535368e-03 9.9664766e-01 8.3928369e-04]\n",
      " [2.2605374e-02 5.0307461e-03 1.7971978e-01 1.1503365e-02 7.8114086e-01]]\n",
      "most likely model\n",
      "sbm\n",
      "lw\n",
      "sbm\n",
      "Ground truth\n",
      "sbm\n",
      "lw\n",
      "attm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"probability of each model class for second segment\",'\\n',cl2_3d)\n",
    "print(\"most likely model\")\n",
    "models = ['attm', 'ctrw', 'fbm', 'lw', 'sbm'] \n",
    "for i in range(len(data)):\n",
    "    print(models[np.argmax(cl2_3d[i])])\n",
    "print('Ground truth')    \n",
    "for i in range(len(data)):\n",
    "    print(models[int(Y3[2][i,2])])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
