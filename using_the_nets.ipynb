{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import andi\n",
    "import numpy as np\n",
    "import csv as csv\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras import losses, metrics\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.layers import LSTM\n",
    "\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Bidirectional\n",
    "#from keras.models import load_model\n",
    "from tensorflow.keras.models import load_model\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "#from data_split import data_split\n",
    "from utils import data_norm, data_reshape, many_net_uhd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "trajs_from_files = csv.reader(open('challenge_for_scoring/task1.txt','r'), delimiter=';', \n",
    "                                        lineterminator='\\n',quoting=csv.QUOTE_NONNUMERIC)\n",
    "validation = [[],[],[]]\n",
    "for trajs in enumerate(trajs_from_files):\n",
    "    validation[int(trajs[1][0])-1].append(trajs[1][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating trajectories to test the data. 3 trajectories for each dimension of inference and classification tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a dataset for task(s) [1, 2] and dimension(s) [1, 2, 3].\n",
      "Generating dataset for dimension 1.\n",
      "Generating dataset for dimension 2.\n",
      "Generating dataset for dimension 3.\n"
     ]
    }
   ],
   "source": [
    "AD = andi.andi_datasets()\n",
    "i=200\n",
    "\n",
    "X1, Y1, X2, Y2, X3, Y3 = AD.andi_dataset(N = 3, tasks = [1,2], dimensions = [1,2,3],\n",
    "                                        min_T = i, max_T = i+1, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference of 1d trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the inference networks is the inferred anomalous exponent for each trajectory. Trajectories must be of equal length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the networks used for inference in 1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers_inf_1d = [25, 50, 65, 75, 125, 165, 225, \n",
    "                  325, 425, 525, 625, 725, 825, 925]\n",
    "meta_model_inf_1d = []\n",
    "for i in centers_inf_1d: \n",
    "    m = load_model('nets/inference_nets/1d/inference_1D_'+str(i)+'.h5')\n",
    "    \n",
    "    meta_model_inf_1d.append(m)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the net trained on trajectories of length 165"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted exponents [1.845595   1.6862895  0.92254865]\n",
      "ground truth [1.8, 1.9, 1.0]\n"
     ]
    }
   ],
   "source": [
    "#choosing the net\n",
    "net = meta_model_inf_1d[5]\n",
    "#finding out the block size used by the chosen net\n",
    "bs = net.layers[0].input_shape[-1]\n",
    "\n",
    "#normalizing the data\n",
    "data = data_norm(X1[0],dim=1,task=1)\n",
    "\n",
    "\n",
    "#reshaping the data\n",
    "\n",
    "data_rs = data_reshape(data,bs=bs,dim=1)\n",
    "#prediction on trajectories of length 200 using a net trained on traj of length 165\n",
    "pred_200_u165 = net.predict(data_rs)\n",
    "print('predicted exponents', pred_200_u165.flatten())\n",
    "print('ground truth', Y1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the net trained on trajectories of length 225"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted exponents [1.8311543  1.6214162  0.86620164]\n",
      "ground truth [1.8, 1.9, 1.0]\n"
     ]
    }
   ],
   "source": [
    "#choosing the net\n",
    "net = meta_model_inf_1d[6]\n",
    "#finding out the block size used by the chosen net\n",
    "bs = net.layers[0].input_shape[-1]\n",
    "\n",
    "#normalizing the data\n",
    "data = data_norm(X1[0],dim=1,task=1)\n",
    "\n",
    "\n",
    "#reshaping the data\n",
    "\n",
    "data_rs = data_reshape(data,bs=bs,dim=1)\n",
    "#prediction on trajectories of length 200 using a net trained on traj of length 225\n",
    "pred_200_u225 = net.predict(data_rs)\n",
    "print('predicted exponents', pred_200_u225.flatten())\n",
    "print('ground truth', Y1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the combination of nearest nets, which in this case is 165 and 225 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted exponents [1.8371712  1.6484468  0.88967955]\n",
      "ground truth [1.8, 1.9, 1.0]\n"
     ]
    }
   ],
   "source": [
    "pred_200_comb = many_net_uhd(nets = meta_model_inf_1d, traj_set = X1[0], centers = centers_inf_1d ,dim = 1, task =1)\n",
    "print('predicted exponents',pred_200_comb)\n",
    "print('ground truth', Y1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification in 2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the classiciation networks is an array giving the probability that the trajectory belongs to a model class. The classes, as in AnDi are [attm,ctrw,fbm,lw,sbm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers_class_2d = [25, 65, 125, 225, 425]\n",
    "meta_model_class_2d = []\n",
    "for i in centers_class_2d: \n",
    "    m = load_model('nets/classification_nets/2d/classification_2D_'+str(i)+'.h5')\n",
    "    \n",
    "    meta_model_class_2d.append(m)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability of each model class \n",
      " [[9.9778748e-01 5.4923461e-05 1.1240288e-05 9.2051545e-05 2.0543423e-03]\n",
      " [8.6530723e-05 9.5793021e-01 4.1861311e-02 1.1974435e-04 2.1988903e-06]\n",
      " [1.1371747e-03 5.9122603e-06 8.0086995e-04 1.2150083e-06 9.9805486e-01]]\n",
      "predicte most likely model\n",
      "attm\n",
      "ctrw\n",
      "sbm\n",
      "Ground truth\n",
      "attm\n",
      "ctrw\n",
      "sbm\n"
     ]
    }
   ],
   "source": [
    "#choosing the net\n",
    "net = meta_model_class_2d[2]\n",
    "#finding out the block size used by the chosen net\n",
    "bs = net.layers[0].input_shape[-1]\n",
    "\n",
    "#normalizing the data\n",
    "data = data_norm(X2[1],dim=2,task=2)\n",
    "\n",
    "\n",
    "#reshaping the data\n",
    "\n",
    "data_rs = data_reshape(data,bs=bs,dim=2)\n",
    "#prediction on trajectories of length 200 using a net trained on traj of length 165\n",
    "cla_200_u125 = net.predict(data_rs)\n",
    "print(\"probability of each model class\",'\\n',cla_200_u125)\n",
    "print(\"predicte most likely model\")\n",
    "models = ['attm', 'ctrw', 'fbm', 'lw', 'sbm'] \n",
    "for i in range(len(data)):\n",
    "    print(models[np.argmax(cla_200_u125[i])])\n",
    "print('Ground truth')    \n",
    "for i in range(len(data)):\n",
    "    print(models[int(Y2[1][i])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining the nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability of each model class \n",
      " [[9.9878442e-01 5.5976030e-05 2.5782732e-05 2.0775158e-04 9.2605810e-04]\n",
      " [2.0063552e-04 8.5220039e-01 1.4750800e-01 7.0587907e-05 2.0408525e-05]\n",
      " [1.5283580e-03 5.0323297e-06 5.1123847e-04 9.1584417e-07 9.9795437e-01]]\n",
      "most likely model\n",
      "attm\n",
      "ctrw\n",
      "sbm\n",
      "Ground truth\n",
      "attm\n",
      "ctrw\n",
      "sbm\n"
     ]
    }
   ],
   "source": [
    "cla_200_comb = many_net_uhd(nets = meta_model_class_2d, traj_set = X2[1], centers = centers_class_2d ,dim = 2, task =2)\n",
    "cla_200_comb = cla_200_comb.reshape(-1,5)\n",
    "print(\"probability of each model class\",'\\n',cla_200_comb)\n",
    "print(\"most likely model\")\n",
    "models = ['attm', 'ctrw', 'fbm', 'lw', 'sbm'] \n",
    "for i in range(len(data)):\n",
    "    print(models[np.argmax(cla_200_comb[i])])\n",
    "print('Ground truth')    \n",
    "for i in range(len(data)):\n",
    "    print(models[int(Y2[1][i])])    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
