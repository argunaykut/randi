{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "import scipy.io as sci\n",
    "import andi \n",
    "AD = andi.andi_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Generating data for training neural network for inference #########\n",
    "'''\n",
    "Define dimension, number of trajectories for each dataset, trajectory length and number of datasets to be generated\n",
    "'''\n",
    "dimension = 1                # 1, 2 or 3 Dimensions\n",
    "N = 100000                   # Number of trajectories per datasets\n",
    "traj_length = 225            # Length of the trajectories\n",
    "number_dataset = 30          # Number of datasets to be saved\n",
    "\n",
    "n=0\n",
    "for repeat in range(number_dataset): \n",
    "    X, Y, NA, NA, NA, NA = AD.andi_dataset(N = N, tasks = 1, dimensions = dimension,\n",
    "                                             min_T = traj_length, max_T = traj_length+1,)\n",
    "    sci.savemat(r'data\\inference\\ ' + str(dimension) + 'D_' + str(traj_length) + '_' + str(n) + '.mat',{'X': X, 'Y':Y})\n",
    "    n += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Building the recurrent neural network #####\n",
    "\n",
    "model_inference = Sequential()\n",
    "\n",
    "block_size = 4*dimension                                   # Size of the blocks of data points\n",
    "\n",
    "model_inference.add(LSTM(250,                              # first layer: LSTM of dimension 250\n",
    "                         return_sequences=True,            # return sequences for the second LSTM layer            \n",
    "                         recurrent_dropout=0.2,            # recurrent dropout for preventing overtraining\n",
    "                         input_shape=(None, block_size)))  # input shape\n",
    "                                                           \n",
    "model_inference.add(LSTM(50,                               # second layer: LSTM of dimension 50\n",
    "                        dropout=0,\n",
    "                        recurrent_dropout=0.2))\n",
    "\n",
    "model_inference.add(Dense(1))                              # output \n",
    "\n",
    "model_inference.compile(optimizer='adam',\n",
    "                        loss='mse', \n",
    "                        metrics=['mae'])\n",
    "\n",
    "model_inference.summary()                                  # Printing a summary of the built network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Define the function for preprocessing the data\n",
    "\n",
    "def data_prepare(X,Y,N,traj_length,dimension):                # regularize trajectories for training\n",
    "    import numpy as np \n",
    "    thr=1e-10\n",
    "    r = np.array(X).reshape(N,dimension,traj_length)              \n",
    "    r = np.diff(r,axis=2)\n",
    "    x = np.zeros((N,0))\n",
    "    for dim in range(dimension):\n",
    "        y = r[:,dim,:]\n",
    "        sy = np.std(y,axis=1)\n",
    "        y = (y-np.mean(y,axis=1).reshape(len(y),1)) / np.where(sy>thr,sy,1).reshape(len(y),1)   # normalize x data\n",
    "        y = np.concatenate((y,np.zeros((N,1))),axis=1)\n",
    "        x = np.concatenate((x,y),axis=1)                   # merge dimensions\n",
    "    x = np.transpose(x.reshape(N,dimension,traj_length),axes = [0,2,1])\n",
    "    \n",
    "    label = Y\n",
    "    \n",
    "    return(x, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Training the recurrent neural network #####\n",
    "\n",
    "batch_sizes = [32, 128, 512, 2048]\n",
    "dataset_used = [1, 4, 5, 20]\n",
    "number_epochs = [5, 4, 3, 2]\n",
    "n = 0\n",
    "\n",
    "for batch in range(len(batch_sizes)):    \n",
    "    for repeat in range(dataset_used[batch]):\n",
    "        data = sci.loadmat(r'data\\inference\\ ' + str(dimension) + 'D_' + str(traj_length) + '_' + str(n) + '.mat')\n",
    "        n += 1\n",
    "        X = data['X'][0][dimension-1]\n",
    "        Y = data['Y'][0][dimension-1].reshape(N,)\n",
    "        x, label = data_prepare(X,Y,N,traj_length,dimension)\n",
    "        model_inference.fit(x.reshape(N,int(traj_length/block_size),block_size),\n",
    "                            label, \n",
    "                            epochs=number_epochs[batch], \n",
    "                            batch_size=batch_sizes[batch],\n",
    "                            validation_split=0.1,\n",
    "                            shuffle=True)\n",
    "\n",
    "model_inference.save('nets\\user_trained\\inference_' + str(traj_length) + '.h5')     # Save the network "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
