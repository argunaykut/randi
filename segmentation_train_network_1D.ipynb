{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "import scipy.io as sci\n",
    "\n",
    "def data_prepare(X,Y,N,traj_length):               # regularize trajectories for training of the inference values\n",
    "    import numpy as np \n",
    "    thr=1e-10                            \n",
    "    x = np.array(X).reshape(N,traj_length)             # reshape\n",
    "    x = np.diff(x,axis=1)                              # take the increments \n",
    "    sx = np.std(x,axis=1)                 \n",
    "    x = (x-np.mean(x,axis=1).reshape(len(x),1)) / np.where(sx>thr,sx,1).reshape(len(x),1)   # normalize data\n",
    "    \n",
    "    label_inf=np.zeros((N,4))\n",
    "    label_inf[:,0]=Y[:,3]                              # the first exponent\n",
    "    label_inf[:,1]=Y[:,5]                              # the second exponent\n",
    "    label_inf[:,2]=np.sin((2*np.pi*Y[:,1])/200)        # sine of the switching time\n",
    "    label_inf[:,3]=np.cos((2*np.pi*Y[:,1])/200)        # cosine of the switching time\n",
    "    \n",
    "    label_c1 = []\n",
    "    label_c1.append(np.equal(Y[:,2],0))                # if the first model is attm\n",
    "    label_c1.append(np.equal(Y[:,2],1))                # if the first model is ctrw\n",
    "    label_c1.append(np.equal(Y[:,2],2))                # if the first model is sbm\n",
    "    label_c1.append(np.equal(Y[:,2],3))                # if the first model is lw\n",
    "    label_c1.append(np.equal(Y[:,2],4))                # if the first model is fbm\n",
    "    label_c1 = np.array(np.transpose(label)) + 0                            \n",
    "\n",
    "    label_c2 = []\n",
    "    label_c2.append(np.equal(Y[:,2],0))                # if the second model is attm\n",
    "    label_c2.append(np.equal(Y[:,2],1))                # if the second model is ctrw\n",
    "    label_c2.append(np.equal(Y[:,2],2))                # if the second model is sbm\n",
    "    label_c2.append(np.equal(Y[:,2],3))                # if the second model is lw\n",
    "    label_c2.append(np.equal(Y[:,2],4))                # if the second model is fbm\n",
    "    label_c2 = np.array(np.transpose(label)) + 0                            \n",
    "    \n",
    "    return(r, label_inf, label_c1, label_c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Building the recurrent neural networks #####\n",
    "\n",
    "block_size = 4                                             # Size of the blocks of data points\n",
    "\n",
    "###### Building the recurrent neural network for inference #####\n",
    "\n",
    "model_segmentation_inf = Sequential()\n",
    "\n",
    "model_segmentation_inf.add(LSTM(250,                       # first layer: LSTM of dimension 250\n",
    "                         return_sequences=True,            # return sequences for the second LSTM layer            \n",
    "                         recurrent_dropout=0.2,            # recurrent dropout for preventing overtraining\n",
    "                         input_shape=(None, block_size)))  # input shape\n",
    "                                                           \n",
    "model_segmentation_inf.add(LSTM(50,                          # second layer: LSTM of dimension 50\n",
    "                        dropout=0,\n",
    "                        recurrent_dropout=0.2))\n",
    "\n",
    "model_segmentation_inf.add(Dense(20))                        # dense layer \n",
    "\n",
    "model_segmentation_inf.add(Dense(4))                         # output layer                             \n",
    "\n",
    "model_segmentation_inf.compile(optimizer='adam',\n",
    "                               loss='mse', \n",
    "                               metrics=['mae'])\n",
    "\n",
    "model_segmentation_inf.summary()                             # Printing a summary of the built network\n",
    "\n",
    "\n",
    "###### Building the recurrent neural network for classifying the first model #####\n",
    "\n",
    "model_segmentation_c1 = Sequential()\n",
    "\n",
    "model_segmentation_c1.add(LSTM(250,                       # first layer: LSTM of dimension 250\n",
    "                         return_sequences=True,            # return sequences for the second LSTM layer            \n",
    "                         recurrent_dropout=0.2,            # recurrent dropout for preventing overtraining\n",
    "                         input_shape=(None, block_size)))  # input shape\n",
    "                                                           \n",
    "model_segmentation_c1.add(LSTM(50,                          # second layer: LSTM of dimension 50\n",
    "                        dropout=0,\n",
    "                        recurrent_dropout=0.2))\n",
    "\n",
    "model_segmentation_c1.add(Dense(20))                        # dense layer \n",
    "\n",
    "model_segmentation_c1.add(Dense(5,                         # output layer\n",
    "                                activation=\"softmax\",))\n",
    "\n",
    "model_segmentation_c1.compile(optimizer='adam',\n",
    "                              loss=\"categorical_crossentropy\",\n",
    "                              metrics=[\"categorical_accuracy\"])\n",
    "\n",
    "model_segmentation_c1.summary()                             # Printing a summary of the built network\n",
    "\n",
    "\n",
    "\n",
    "###### Building the recurrent neural network for classifying the second model #####\n",
    "\n",
    "model_segmentation_c2 = Sequential()\n",
    "\n",
    "model_segmentation_c2.add(LSTM(250,                       # first layer: LSTM of dimension 250\n",
    "                         return_sequences=True,            # return sequences for the second LSTM layer            \n",
    "                         recurrent_dropout=0.2,            # recurrent dropout for preventing overtraining\n",
    "                         input_shape=(None, block_size)))  # input shape\n",
    "                                                           \n",
    "model_segmentation_c2.add(LSTM(50,                          # second layer: LSTM of dimension 50\n",
    "                        dropout=0,\n",
    "                        recurrent_dropout=0.2))\n",
    "\n",
    "model_segmentation_c2.add(Dense(20))                        # dense layer \n",
    "\n",
    "model_segmentation_c2.add(Dense(5,                          # output layer\n",
    "                                activation=\"softmax\",))\n",
    "\n",
    "model_segmentation_c2.compile(optimizer='adam',\n",
    "                              loss=\"categorical_crossentropy\",\n",
    "                              metrics=[\"categorical_accuracy\"])\n",
    "\n",
    "model_segmentation_c2.summary()                             # Printing a summary of the built network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###### Training the recurrent neural networks #####\n",
    "\n",
    "dimension = 1                # 1, 2 or 3 Dimensions\n",
    "N = 100000                   # Number of trajectories per datasets\n",
    "traj_length = 225            # Length of the trajectories\n",
    "\n",
    "batch_sizes = [32, 128, 512, 2048]\n",
    "dataset_used = [1, 4, 5, 20]\n",
    "number_epochs = [5, 4, 3, 2]\n",
    "n = 0\n",
    "\n",
    "for batch in range(len(batch_sizes)):    \n",
    "    for repeat in range(dataset_used[batch]):\n",
    "        data = sci.loadmat(r'data\\segmentation\\ ' + str(dimension) + 'D_' + str(traj_length) + '_' + str(n) + '.mat')\n",
    "        n += 1\n",
    "        X = data['X'][0][0]\n",
    "        Y = data['Y'][0][0].reshape(N,)\n",
    "        \n",
    "        x, label_inf, label_c1, label_c2 = data_prepare(X,Y,N,traj_length)\n",
    "        \n",
    "        model_segmentation_inf.fit(x.reshape(N,int(traj_length/block_size),block_size),\n",
    "                                 label_inf, \n",
    "                                 epochs=number_epochs[batch], \n",
    "                                 batch_size=batch_sizes[batch],\n",
    "                                 validation_split=0.1,\n",
    "                                 shuffle=True)\n",
    "        \n",
    "        model_segmentation_c1.fit(x.reshape(N,int(traj_length/block_size),block_size),\n",
    "                                 label_c1, \n",
    "                                 epochs=number_epochs[batch], \n",
    "                                 batch_size=batch_sizes[batch],\n",
    "                                 validation_split=0.1,\n",
    "                                 shuffle=True)\n",
    "        \n",
    "        model_segmentation_c2.fit(x.reshape(N,int(traj_length/block_size),block_size),\n",
    "                                 label_c2, \n",
    "                                 epochs=number_epochs[batch], \n",
    "                                 batch_size=batch_sizes[batch],\n",
    "                                 validation_split=0.1,\n",
    "                                 shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
